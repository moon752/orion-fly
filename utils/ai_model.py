# Autoâ€‘generated by ORION on 2025-06-16T19:25:15.938087

def main():
    print("ðŸ”§ utils/ai_model.py ready")

# ---------- Fireworks helper ----------
import requests, time
_FW_LAST = {}
def _fw_throttle(k):
    now=time.time()
    if k in _FW_LAST and now-_FW_LAST[k] < 1: time.sleep(1-(now-_FW_LAST[k]))
    _FW_LAST[k]=time.time()

def _fw_chat(key, messages, model="llama-v3-8b-instruct", temp=0.7, mx=512):
    _fw_throttle(key)
    r=requests.post("https://api.fireworks.ai/v1/chat/completions",
        headers={"Authorization":f"Bearer {key}"},
        json={"model":model,"messages":messages,"temperature":temp,
              "max_tokens":mx,"stream":False},
        timeout=40)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]


# --- Fireworks AI integration ---
import os
from openai import OpenAI as _FWOpenAI

FW_CLIENT = _FWOpenAI(
    base_url=os.getenv("FIREWORKS_API_BASE", "https://api.fireworks.ai/inference/v1"),
    api_key=os.getenv("FIREWORKS_API_KEY")
)

def call_fireworks(messages, model, temp=0.7):
    resp = FW_CLIENT.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temp,
        stream=False
    )
    return resp.choices[0].message.content

# Wrap into ai.chat fallback
_original_chat = ai.chat
def chat_with_fw(messages, model=None):
    try:
        return call_fireworks(messages, model or "accounts/fireworks/models/deepseek-r1")
    except Exception:
        return _original_chat(messages)
ai.chat = chat_with_fw
